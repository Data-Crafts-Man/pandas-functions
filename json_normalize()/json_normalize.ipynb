{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# json_normalize()\n",
    "* In this video, we will talk about how to use the pandas json_normalize function and go in depth on the arguments you can pass. \n",
    "* Within the video description, I have included a link to the github repository that has this jupyter notebook and a link to the dataset I'm using in this video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opening JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open json as a text file \n",
    "f = open('sea_level_rise.json','r')\n",
    "\n",
    "# convert to python dictionary \n",
    "file = json.loads(f.read())\n",
    "print(type(file))\n",
    "# file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigating JSON Files\n",
    "\n",
    "* When I say navigating the JSON file what we are actually doing is navigating a python dictionary version of the json file.\n",
    "* Dictionaries in python, just like json are based on a key and value pair logic. \n",
    "* Central to naviagating any JSON file, therefore is knowing what all of the keys are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Level \n",
    "file.keys()\n",
    "# Many JSON data files are initally divided into two main parts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Level\n",
    "file['meta'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third Level\n",
    "file['meta']['view'].keys()\n",
    "# Eventually you will get to the point where you dont have keys anymore. you will just have values or lists of values (arrays). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file['meta']['view']['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Arguments\n",
    "* The json_normalize function attempts to create an easy to read pandas dataframe. This is an extremely useful tool and there are multiple arguments you can use to produce a really clean output. \n",
    "* Let's first use an example with no arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.json_normalize(file)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# record_path argument\n",
    "* Now we will use the record path argument to step into this data a little bit more. \n",
    "* record_path can take either a string or list of strings. Accpetable strings are keys within the json file.\n",
    "* When passing a single key a data frame will be generated from that level. \n",
    "* When passing a list of keys you are specifying the exact level and location you want the dataframe to be generated from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.json_normalize(file,record_path='data')\n",
    "data\n",
    "# You can see that this dataframe does not have any column headers, but those can be found in the meta data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Column List\n",
    "* In order to generate the column header list for the dataframe above, we will use the json_normalize function again.\n",
    "* This time we will be using an extended record path by passing in a list of strings rather than a single string.\n",
    "* All column names are found in the meta data so we will make the walk into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = pd.json_normalize(file,record_path=['meta','view','columns'])\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the dataframe, the \"name\" column apprears to contain the column names we need for our main dataframe. \n",
    "# We will grab the entire column and convert it to a dictionary. \n",
    "col_list = cols['name'].to_dict()\n",
    "col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the column names, we can change the column headers in the dataframe with the rename function. \n",
    "# Unfortunately, unlike read_csv there is no way to pass the column names within the json_normalize function. So you must do it after the fact. \n",
    "data.rename(columns=col_list,inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# meta argument\n",
    "* We can add additional rows to our dataframe using the meta data section and meta argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.json_normalize(file,record_path='data',meta='meta')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# meta_prefix argument\n",
    "* Up to this point we have used a json file where there were two main keys on the first level, but not all json files will be like that. Rather than having two main keys like \"data\" and \"meta\", you may have multiple keys on the first level creating what is known as a flatter json structure. \n",
    "* Assuming you want to include meta data, you can use the meta_prefix argument to help you organize the additional columns.\n",
    "* We are going to use a modified version of the sea_level_rise.json file. I removed the meta and view keys so that all meta keys are on the same level as the data key. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open json as a text file \n",
    "f2 = open('sea_level_rise_flat.json','r')\n",
    "\n",
    "# convert to python dictionary \n",
    "file2 = json.loads(f2.read())\n",
    "print(type(file2))\n",
    "# file2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigating the Modified File\n",
    "* Now that we have this \"flatter\" version in a dictionary object, lets explore it like we did the first using the keys() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file2.keys()\n",
    "# Notice in this adjusted files that there are more keys at the inital level when compared with the other file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.json_normalize(file2,record_path='data',meta=['name','category','metadata'],meta_prefix='meta.')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# record_prefix argument\n",
    "* In addition to adding a meta prefix, you can add record prefixes too.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.json_normalize(file2,record_path='data',meta=['name','category'],meta_prefix='meta.',record_prefix='data.')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# errors argument\n",
    "* The default behavior for the errors argument is to raise errors and exit the execution.\n",
    "* This occurs when meta key valuesare missing. \n",
    "* To avoid this, you can set the error argument to \"ignore\". \n",
    "* When you do this np.nan values are placed in the missing spots and the dataframe can still be created successfully. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.json_normalize(file2,record_path='data',meta=['name','category','nonExistentMeta'],meta_prefix='meta.',record_prefix='data.')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.json_normalize(file2,record_path='data',meta=['name','category','nonExistentMeta'],meta_prefix='meta.',record_prefix='data.',errors='ignore')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sep argument\n",
    "* Going back to the original file, we will now demonstrate how to use the sep argument.\n",
    "* sep allows you to change the path separator.\n",
    "* While this may look similar to prefixes, there is a fundamental difference. \n",
    "* Prefixes overwrite the JSON defined paths and are used for organizing \"flatter\" json files. Separators are JSON defined paths used to show where the data is coming from in \"hierarchical\" json files. \n",
    "* Remember back to the first example we did with no arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.json_normalize(file)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.json_normalize(file,sep='>>>')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# max_level argument\n",
    "* By default json normalize goes through all levels of the json file. \n",
    "* But you can specify how far you would like it to go. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.json_normalize(file,max_level=0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.json_normalize(file,max_level=2)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.json_normalize(file,max_level=3)\n",
    "data\n",
    "#NOTE: the deeper you go, the more columns you will get. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternate way to get column headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file['meta']['view']['columns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = []\n",
    "for name in file['meta']['view']['columns']:\n",
    "    col_list.append(name['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
